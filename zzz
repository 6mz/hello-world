# -*- coding: utf-8 -*-
split.py
"""
import os, time, pickle, random, time
from datetime import datetime
import numpy as np
import math
from imageio import imwrite
import utils_align as utils_align

import glob
import cv2

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def analyse_raw_name(raw_name):
    raw_items = os.path.basename(raw_name).split("_")
    for i in range(len(raw_items)):
        if "ISO" == raw_items[i]:
            iso = int(raw_items[i+1])
    return iso

def analyse_ce_raw_name(raw_name):
    raw_items = os.path.basename(raw_name).split("_")
    iso = int(raw_items[5])

    return iso



def read_vogue_raw(raw_file, size):
    raw = np.fromfile(raw_file, dtype=np.uint16)
    raw = raw.reshape(size)
    
    #raw = raw_file
    row_num = raw.shape[0]
    column_num = raw.shape[1]

    img_b = raw[0:row_num:2, 0:column_num:2]
    img_gb = raw[0:row_num:2, 1:column_num:2]
    img_gr = raw[1:row_num:2, 0:column_num:2]
    img_r = raw[1:row_num:2, 1:column_num:2]

    img_test = np.stack([img_b, img_gb, img_gr, img_r], axis=-1)

    img_test = img_test / 1023.

    return img_test

def read_simple_raw(raw_file,size):
    raw = np.fromfile(raw_file,dtype=np.uint16)
    raw = raw.reshape(size)
    raw = raw / 1023.
    return raw


def crop_raw(raw,row,col):
    crop = raw[150:150+row-1,150:150+col-1]
    return crop

def calc_matrix(img1, img2):
    #first, find the sift features in images and apply the ratio test to find the best matches
    MIN_MATCH_COUNT = 10
    sift = cv2.xfeatures2d.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1,None)
    kp2, des2 = sift.detectAndCompute(img2,None)
    FLANN_INDEX_KDTREE = 0
    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
    search_params = dict(check = 50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matchs = flann.knnMatch(des1,des2,k=2)	
    good = []
    for m,n in matchs:
    	if m.distance < 0.7*n.distance:
    		good.append(m)
    	
    #extract the locations of matched keypoints in both the images, and get the 3*3 transformation matrix
    if len(good)>MIN_MATCH_COUNT:
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)
        #return M
    else:
        M = np.diag(np.diag(np.ones([3,3])))
    return M
    
def regis(raw,raw_tmp,shape):
    homo = calc_matrix(raw_tmp,raw)  #we need to warp raw_tmp to the position of raw
    raw_tmp = cv2.warpPerspective(raw_tmp,homo,shape,cv2.INTER_LINEAR,cv2.BORDER_REPLICATE)
    return raw_tmp
    
#10bit
def save_10bit_raw(input, path, size):
    row_num = size[0]
    column_num = size[1]
    rst_raw = np.ndarray(shape=size, dtype=np.float32)
    #print(rst_raw.shape)
    rst_raw[1:row_num:2, 1:column_num:2] = input[ :, :, 3]
    rst_raw[1:row_num:2, 0:column_num:2] = input[ :, :, 2]
    rst_raw[0:row_num:2, 1:column_num:2] = input[ :, :, 1]
    rst_raw[0:row_num:2, 0:column_num:2] = input[ :, :, 0]
    rst_raw = rst_raw * 1023

    rst_raw = np.where(rst_raw > 1023, 1023, rst_raw)
    rst_raw = rst_raw.astype(np.uint16)
    rst_raw.tofile(path)

    return 0

#10bit
def save_10bit_simple(input, path, size):
    #row_num = size[0]
    #column_num = size[1]
    rst_raw = np.ndarray(shape=size, dtype=np.float32)
    rst_raw = input * 1023.
    rst_raw = np.where(rst_raw > 1023, 1023, rst_raw)
    rst_raw = rst_raw.astype(np.uint16)
    rst_raw.tofile(path)

    return 0



def evaluate_net_v2(src_path, result_raw_path, folder_depth, test_items):

    # size= (2976, 3968)
    size = (2736, 3648)
    #size = (3648,2736)
    shape = (1824, 1368)

    src_folder_list = []
    dst_raw_path_list = []
    # for depth in folder_depth_list:
    #     tmp = sorted(glob.glob(src_path + "*/"*depth + "data_noise"))
    #     folder_list.extend(tmp)

    # test data bench mark
    for item in test_items:
        tmp = sorted(glob.glob(src_path + item + "*/"*folder_depth))
        src_folder_list.extend(tmp)
    for i in range(len(src_folder_list)):
        dst_raw_path_list.append(result_raw_path + src_folder_list[i][len(src_path):])

    reuse_flag = False

    loaded = False
    for i in range(len(src_folder_list)):
        print(src_folder_list[i])
        raw_files = glob.glob(src_folder_list[i] + '*.raw')
        raw_files.sort()
        raw_name = raw_files[0].split('/')[-1]
        prefix = raw_name.split('_')[0]
        
        create_dir(dst_raw_path_list[i])
        dst_raw_name = dst_raw_path_list[i] + prefix + '_' + raw_name[len(prefix):]

        frm_num = 4
        '''
        orig_raw_0 = read_vogue_raw(raw_files[frm_num-1],size)
        #print(orig_raw1.shape)
        save_10bit_raw(orig_raw_0, dst_raw_name, size)
        imwrite(dst_raw_path_list[i]+'_g_%d.png'% (frm_num-1), orig_raw_0[:,:,2])
        orig_raw_0 = np.clip(orig_raw_0, 0, 1)
        orig_raw_0 = orig_raw_0 * 255
        orig_raw_0 = orig_raw_0.astype(np.uint8)
        start_time = time.time()
        
        for f_tmp in range(0, frm_num-1):
            orig_raw_tmp = read_vogue_raw(raw_files[f_tmp],size)
             
            orig_raw_tmp = np.clip(orig_raw_tmp, 0, 1)
            orig_raw_tmp = 255*orig_raw_tmp
            orig_raw_tmp = orig_raw_tmp.astype(np.uint8)
            #orig_raw_tmp = regis(orig_raw_0, orig_raw_tmp1,size_3)
            
            homo = calc_matrix(orig_raw_0*2, orig_raw_tmp*2)  #we need to warp raw_tmp to the position of raw
            orig_raw_tmp = cv2.warpPerspective(orig_raw_tmp, homo, shape, cv2.INTER_LINEAR, cv2.BORDER_REPLICATE)
            
            
            orig_raw_tmp = orig_raw_tmp/(255.0)
            raw_name = raw_files[f_tmp+1].split('/')[-1]
            prefix = raw_name.split('_')[0]
            dst_raw_name = dst_raw_path_list[i] + prefix + '_' + raw_name[len(prefix):]
            save_10bit_raw(orig_raw_tmp, dst_raw_name, size)
            imwrite(dst_raw_path_list[i]+'_g_%d.png'% f_tmp, orig_raw_tmp[:,:,2])
            print(orig_raw_tmp.shape)

            print("process folder:%s" % src_folder_list[i])
            print("took: %4.4fs" % (time.time() - start_time))
            '''
        MOTION_MODEL = 'ECC'
        orig_raw = []
        orig_raw_G = []
        for f_tmp in range(0, frm_num):
            orig_raw_tmp = read_vogue_raw(raw_files[f_tmp],size)
            orig_raw_tmp = 255*orig_raw_tmp
            orig_raw_tmp = orig_raw_tmp.astype(np.uint8)
            orig_raw.append(orig_raw_tmp)
            orig_raw_G.append(orig_raw_tmp[:,:,1]*2)
        print('Start alignment')
        start_time = time.time()
        if MOTION_MODEL == 'ECC':
            t, t_inv, valid_id = utils_align.align_ecc(orig_raw, orig_raw_G, 0, thre=0.3)
        elif MOTION_MODEL == 'RIGID':
            t, t_inv, valid_id = utils_align.align_rigid(orig_raw, orig_raw_G, 0, thre=0.2)
        print("Full alignment: %4.4f" %(time.time() - start_time))
        
        orig_raw = [v.astype(np.float32)/255.0 for v in orig_raw]
        images_t, t, t_inv = utils_align.apply_transform(orig_raw, t, t_inv, MOTION_MODEL, scale=1)
        
        #crop
        is_crop = False#True
        if is_crop == True:
            min_h, max_h, min_w, max_w = 50, size[0]//2-50, 50, size[1]//2-50
            out_size = (2*(max_h-min_h), 2*(max_w-min_w))
        else:
            min_h, max_h, min_w, max_w = 0, size[0], 0, size[1]
            out_size = size
        ###
        for f_tmp in range(0, frm_num):
            img_tmp = images_t[f_tmp]
            
            img_tmp = img_tmp[min_h:max_h, min_w:max_w,:]
            raw_name = raw_files[f_tmp].split('/')[-1]
            prefix = raw_name.split('_')[0]
            dst_raw_name = dst_raw_path_list[i] + prefix + '_' + raw_name[len(prefix):]
            save_10bit_raw(img_tmp, dst_raw_name, out_size)
            imwrite(dst_raw_path_list[i]+MOTION_MODEL+'_g_%d.png'% f_tmp, img_tmp[:,:,2]*2)
        

    return 0

if __name__ == "__main__":
    test_items = ["20190703/"]
    #test_items = ["good/"]
    #test_items = ["test/"]
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"


    src_path = '/DATA1/dlnr_zcz/test_data/'#Log_20190322_1737/'

    #result_raw_path = '/DATA1/dlnr_zcz/test_data/after_registration/'
    result_raw_path = '/DATA1/dlnr_zcz/test_data/registration/20190703/input/'

    #evaluate_net(src_path, result_raw_path, gen_raw, gen_png, 1, net_dict, test_items)
    evaluate_net_v2(src_path, result_raw_path, 1, test_items)
    
    
    
    
    
##################    ultils_align.py######################
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import cv2
import numpy as np

def image_uint8(image):
    if image.max() > 10:
        return image
    image = (image * 255).astype(np.uint8)
    return image

def align_rigid(image_set, images_gray_set, ref_ind, thre=0.05):
    img_num = len(image_set)
    ref_gray_image = images_gray_set[ref_ind]
    r, c = image_set[0].shape[0:2]

    identity_transform = np.eye(2, 3, dtype=np.float32)
    warp_matrix = np.eye(2, 3, dtype=np.float32)
    tform_set_init = [np.eye(2, 3, dtype=np.float32)] * img_num

    tform_set = np.zeros_like(tform_set_init)
    tform_inv_set = np.zeros_like(tform_set_init)
    valid_id = []
    motion_thre = thre * min(r, c)
    for i in range(ref_ind - 1, -1, -1):
        warp_matrix = cv2.estimateRigidTransform(image_uint8(ref_gray_image), 
            image_uint8(images_gray_set[i]), fullAffine=0)
        if warp_matrix is None:
            continue
        tform_set[i] = warp_matrix
        tform_inv_set[i] = cv2.invertAffineTransform(warp_matrix)

        motion_val = abs(warp_matrix - identity_transform).sum()
        if motion_val < motion_thre:
            valid_id.append(i)
        else:
            continue

    warp_matrix = np.eye(2, 3, dtype=np.float32)
    for i in range(ref_ind, img_num, 1):
        warp_matrix = cv2.estimateRigidTransform(image_uint8(ref_gray_image), 
            image_uint8(images_gray_set[i]), fullAffine=0)
        if warp_matrix is None:
            tform_set[i] = identity_transform
            tform_inv_set[i] = identity_transform
            continue
        tform_set[i] = warp_matrix
        tform_inv_set[i] = cv2.invertAffineTransform(warp_matrix)
        motion_val = abs(warp_matrix - identity_transform).sum()
        if motion_val < motion_thre:
            valid_id.append(i)
        else:
            continue
    return tform_set, tform_inv_set, valid_id

def align_ecc(image_set, images_gray_set, ref_ind, thre=0.05):
    img_num = len(image_set)
    ref_gray_image = images_gray_set[ref_ind]
    r, c = image_set[0].shape[0:2]

    warp_mode = cv2.MOTION_AFFINE
    # cv2.MOTION_HOMOGRAPHY # cv2.MOTION_AFFINE # cv2.MOTION_TRANSLATION # cv2.MOTION_EUCLIDEAN

    # Define 2x3 or 3x3 matrices and initialize the matrix to identity
    if  warp_mode == cv2.MOTION_HOMOGRAPHY:
        print("Using homography model for alignment")
        identity_transform = np.eye(3, 3, dtype=np.float32)
        warp_matrix = np.eye(3, 3, dtype=np.float32)
        tform_set_init = [np.eye(3, 3, dtype=np.float32)] * img_num
    else:
        identity_transform = np.eye(2, 3, dtype=np.float32)
        warp_matrix = np.eye(2, 3, dtype=np.float32)
        tform_set_init = [np.eye(2, 3, dtype=np.float32)] * img_num

    number_of_iterations = 500
    termination_eps = 1e-6
    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations, termination_eps)

    # Run the ECC algorithm. The results are stored in warp_matrix.
    tform_set = np.zeros_like(tform_set_init)
    tform_inv_set = np.zeros_like(tform_set_init)
    valid_id = []
    motion_thre = thre * min(r, c)
    for i in range(ref_ind - 1, -1, -1):
        _, warp_matrix = cv2.findTransformECC(ref_gray_image, images_gray_set[i], warp_matrix, warp_mode, criteria)
        tform_set[i] = warp_matrix
        tform_inv_set[i] = cv2.invertAffineTransform(warp_matrix)

        motion_val = abs(warp_matrix - identity_transform).sum()
        if motion_val < motion_thre:
            valid_id.append(i)
        else:
            continue

    if  warp_mode == cv2.MOTION_HOMOGRAPHY:
        warp_matrix = np.eye(3, 3, dtype=np.float32)
    else:
        warp_matrix = np.eye(2, 3, dtype=np.float32)

    for i in range(ref_ind, img_num, 1):
        _, warp_matrix = cv2.findTransformECC(ref_gray_image, images_gray_set[i], warp_matrix, warp_mode, criteria)
        tform_set[i] = warp_matrix
        tform_inv_set[i] = cv2.invertAffineTransform(warp_matrix)

        motion_val = abs(warp_matrix - identity_transform).sum()
        if motion_val < motion_thre:
            valid_id.append(i)
        else:
            continue
    return tform_set, tform_inv_set, valid_id

def apply_transform(image_set, tform_set, tform_inv_set, t_type, scale=1.):
    tform_set_2 = tform_set
    tform_inv_set_2 = tform_inv_set
    if t_type is None:
        if tform_set[0].shape == 2:
            t_type = "rigid"
        elif tform_set[0].shape == 3:
            t_type = "homography"
        else:
            print("[x] Invalid transforms")
            exit()

    r, c = image_set[0].shape[0:2]
    img_num = len(image_set)
    image_t_set = np.zeros_like(image_set)
    for i in range(img_num):
        image_i = image_set[i]
        tform_i = tform_set[i]
        tform_i_inv = tform_inv_set[i]
        tform_i[0,2] *= scale
        tform_i[1,2] *= scale
        tform_i_inv[0,2] *= scale
        tform_i_inv[1,2] *= scale
        tform_set_2[i] = tform_i
        tform_inv_set_2[i] = tform_i_inv
        if t_type != "homography":
            image_i_transform = cv2.warpAffine(image_i, tform_i, (c, r),
                                                flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)
        else:
            image_i_transform = cv2.warpPerspective(image_i, tform_i, (c, r),
                                                flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)
        image_t_set[i] = image_i_transform

    return image_t_set, tform_set_2, tform_inv_set_2

def sum_aligned_image(image_aligned, image_set):
    sum_img = np.float32(image_set[0]) * 1. / len(image_aligned)
    sum_img_t = np.float32(image_aligned[0]) * 1. / len(image_aligned)
    identity_transform = np.eye(2, 3, dtype=np.float32)
    r, c = image_set[0].shape[0:2]
    for i in range(1, len(image_aligned)):
        sum_img_t += np.float32(image_aligned[i]) * 1. / len(image_aligned)
        image_set_i = cv2.warpAffine(image_set[i], identity_transform, (c, r),
            flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)
        sum_img += np.float32(image_set_i) * 1. / len(image_aligned)
    return sum_img_t, sum_img

############################res.py######################
"""
res.py
"""
import os, time, pickle, random, time
from datetime import datetime
import numpy as np
import math
from imageio import imwrite
import utils_align as utils_align

import glob
import cv2

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def analyse_raw_name(raw_name):
    raw_items = os.path.basename(raw_name).split("_")
    for i in range(len(raw_items)):
        if "ISO" == raw_items[i]:
            iso = int(raw_items[i+1])
    return iso

def analyse_ce_raw_name(raw_name):
    raw_items = os.path.basename(raw_name).split("_")
    iso = int(raw_items[5])

    return iso



def read_vogue_raw(raw_file, size):
    raw = np.fromfile(raw_file, dtype=np.uint16)
    raw = raw.reshape(size)
    
    #raw = raw_file
    row_num = raw.shape[0]
    column_num = raw.shape[1]

    img_b = raw[0:row_num:2, 0:column_num:2]
    img_gb = raw[0:row_num:2, 1:column_num:2]
    img_gr = raw[1:row_num:2, 0:column_num:2]
    img_r = raw[1:row_num:2, 1:column_num:2]

    img_test = np.stack([img_b, img_gb, img_gr, img_r], axis=-1)

    img_test = img_test / 1023.

    return img_test

def read_simple_raw(raw_file,size):
    raw = np.fromfile(raw_file,dtype=np.uint16)
    raw = raw.reshape(size)
    raw = raw / 1023.
    return raw


def crop_raw(raw,row,col):
    crop = raw[150:150+row-1,150:150+col-1]
    return crop

def calc_matrix(img1, img2):
    #first, find the sift features in images and apply the ratio test to find the best matches
    MIN_MATCH_COUNT = 10
    sift = cv2.xfeatures2d.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1,None)
    kp2, des2 = sift.detectAndCompute(img2,None)
    FLANN_INDEX_KDTREE = 0
    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
    search_params = dict(check = 50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matchs = flann.knnMatch(des1,des2,k=2)	
    good = []
    for m,n in matchs:
    	if m.distance < 0.7*n.distance:
    		good.append(m)
    	
    #extract the locations of matched keypoints in both the images, and get the 3*3 transformation matrix
    if len(good)>MIN_MATCH_COUNT:
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)
        #return M
    else:
        M = np.diag(np.diag(np.ones([3,3])))
    return M
    
def regis(raw,raw_tmp,shape):
    homo = calc_matrix(raw_tmp,raw)  #we need to warp raw_tmp to the position of raw
    raw_tmp = cv2.warpPerspective(raw_tmp,homo,shape,cv2.INTER_LINEAR,cv2.BORDER_REPLICATE)
    return raw_tmp
    
#10bit
def save_10bit_raw(input, path, size):
    row_num = size[0]
    column_num = size[1]
    rst_raw = np.ndarray(shape=size, dtype=np.float32)
    #print(rst_raw.shape)
    rst_raw[1:row_num:2, 1:column_num:2] = input[ :, :, 3]
    rst_raw[1:row_num:2, 0:column_num:2] = input[ :, :, 2]
    rst_raw[0:row_num:2, 1:column_num:2] = input[ :, :, 1]
    rst_raw[0:row_num:2, 0:column_num:2] = input[ :, :, 0]
    rst_raw = rst_raw * 1023

    rst_raw = np.where(rst_raw > 1023, 1023, rst_raw)
    rst_raw = rst_raw.astype(np.uint16)
    rst_raw.tofile(path)

    return 0

#10bit
def save_10bit_simple(input, path, size):
    #row_num = size[0]
    #column_num = size[1]
    rst_raw = np.ndarray(shape=size, dtype=np.float32)
    rst_raw = input * 1023.
    rst_raw = np.where(rst_raw > 1023, 1023, rst_raw)
    rst_raw = rst_raw.astype(np.uint16)
    rst_raw.tofile(path)

    return 0



def evaluate_net_v2(src_path, result_raw_path, folder_depth, test_items):

    # size= (2976, 3968)
    size = (2736, 3648)
    #size = (3648,2736)
    shape = (1824, 1368)

    src_folder_list = []
    dst_raw_path_list = []
    # for depth in folder_depth_list:
    #     tmp = sorted(glob.glob(src_path + "*/"*depth + "data_noise"))
    #     folder_list.extend(tmp)

    # test data bench mark
    for item in test_items:
        tmp = sorted(glob.glob(src_path + item + "*/"*folder_depth))
        src_folder_list.extend(tmp)
    for i in range(len(src_folder_list)):
        dst_raw_path_list.append(result_raw_path + src_folder_list[i][len(src_path):])

    reuse_flag = False

    loaded = False
    for i in range(len(src_folder_list)):
        print(src_folder_list[i])
        raw_files = glob.glob(src_folder_list[i] + '*.raw')
        raw_files.sort()
        raw_name = raw_files[0].split('/')[-1]
        prefix = raw_name.split('_')[0]
        
        create_dir(dst_raw_path_list[i])
        dst_raw_name = dst_raw_path_list[i] + prefix + '_' + raw_name[len(prefix):]

        frm_num = 4
        '''
        orig_raw_0 = read_vogue_raw(raw_files[frm_num-1],size)
        #print(orig_raw1.shape)
        save_10bit_raw(orig_raw_0, dst_raw_name, size)
        imwrite(dst_raw_path_list[i]+'_g_%d.png'% (frm_num-1), orig_raw_0[:,:,2])
        orig_raw_0 = np.clip(orig_raw_0, 0, 1)
        orig_raw_0 = orig_raw_0 * 255
        orig_raw_0 = orig_raw_0.astype(np.uint8)
        start_time = time.time()
        
        for f_tmp in range(0, frm_num-1):
            orig_raw_tmp = read_vogue_raw(raw_files[f_tmp],size)
             
            orig_raw_tmp = np.clip(orig_raw_tmp, 0, 1)
            orig_raw_tmp = 255*orig_raw_tmp
            orig_raw_tmp = orig_raw_tmp.astype(np.uint8)
            #orig_raw_tmp = regis(orig_raw_0, orig_raw_tmp1,size_3)
            
            homo = calc_matrix(orig_raw_0*2, orig_raw_tmp*2)  #we need to warp raw_tmp to the position of raw
            orig_raw_tmp = cv2.warpPerspective(orig_raw_tmp, homo, shape, cv2.INTER_LINEAR, cv2.BORDER_REPLICATE)
            
            
            orig_raw_tmp = orig_raw_tmp/(255.0)
            raw_name = raw_files[f_tmp+1].split('/')[-1]
            prefix = raw_name.split('_')[0]
            dst_raw_name = dst_raw_path_list[i] + prefix + '_' + raw_name[len(prefix):]
            save_10bit_raw(orig_raw_tmp, dst_raw_name, size)
            imwrite(dst_raw_path_list[i]+'_g_%d.png'% f_tmp, orig_raw_tmp[:,:,2])
            print(orig_raw_tmp.shape)

            print("process folder:%s" % src_folder_list[i])
            print("took: %4.4fs" % (time.time() - start_time))
            '''
        MOTION_MODEL = 'ECC'
        orig_raw = []
        orig_raw_G = []
        for f_tmp in range(0, frm_num):
            orig_raw_tmp = read_vogue_raw(raw_files[f_tmp],size)
            orig_raw_tmp = 255*orig_raw_tmp
            orig_raw_tmp = orig_raw_tmp.astype(np.uint8)
            orig_raw.append(orig_raw_tmp)
            orig_raw_G.append(orig_raw_tmp[:,:,1]*2)
        print('Start alignment')
        start_time = time.time()
        if MOTION_MODEL == 'ECC':
            t, t_inv, valid_id = utils_align.align_ecc(orig_raw, orig_raw_G, 0, thre=0.3)
        elif MOTION_MODEL == 'RIGID':
            t, t_inv, valid_id = utils_align.align_rigid(orig_raw, orig_raw_G, 0, thre=0.2)
        print("Full alignment: %4.4f" %(time.time() - start_time))
        
        orig_raw = [v.astype(np.float32)/255.0 for v in orig_raw]
        images_t, t, t_inv = utils_align.apply_transform(orig_raw, t, t_inv, MOTION_MODEL, scale=1)
        
        #crop
        is_crop = False#True
        if is_crop == True:
            min_h, max_h, min_w, max_w = 50, size[0]//2-50, 50, size[1]//2-50
            out_size = (2*(max_h-min_h), 2*(max_w-min_w))
        else:
            min_h, max_h, min_w, max_w = 0, size[0], 0, size[1]
            out_size = size
        ###
        for f_tmp in range(0, frm_num):
            img_tmp = images_t[f_tmp]
            
            img_tmp = img_tmp[min_h:max_h, min_w:max_w,:]
            raw_name = raw_files[f_tmp].split('/')[-1]
            prefix = raw_name.split('_')[0]
            dst_raw_name = dst_raw_path_list[i] + prefix + '_' + raw_name[len(prefix):]
            save_10bit_raw(img_tmp, dst_raw_name, out_size)
            imwrite(dst_raw_path_list[i]+MOTION_MODEL+'_g_%d.png'% f_tmp, img_tmp[:,:,2]*2)
        

    return 0

if __name__ == "__main__":
    test_items = ["20190703/"]
    #test_items = ["good/"]
    #test_items = ["test/"]
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"


    src_path = '/DATA1/dlnr_zcz/test_data/'#Log_20190322_1737/'

    #result_raw_path = '/DATA1/dlnr_zcz/test_data/after_registration/'
    result_raw_path = '/DATA1/dlnr_zcz/test_data/registration/20190703/input/'

    #evaluate_net(src_path, result_raw_path, gen_raw, gen_png, 1, net_dict, test_items)
    evaluate_net_v2(src_path, result_raw_path, 1, test_items)
    
    
    
############generate_H.py#####################
import numpy as np
import matplotlib.pyplot as plt
from math import ceil
import os,cv2,time
import tensorflow as tf
frm_num = 80
m_num = frm_num//4
flag=1

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def gen_example(homo_list, shape=(84, 8), m_num=20):
    tfrecords_features = {}
    tfrecords_features['Homo'] = tf.train.Feature(bytes_list=tf.train.BytesList(value=homo_list))
    tfrecords_features['shape'] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(shape)))
    tfrecords_features['m_num'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[m_num]))
    return tf.train.Example(features=tf.train.Features(feature=tfrecords_features))
	

def gen_example_v2(homo_list, shape=(1, 8), m_num=1):
    tfrecords_features = {}
    tfrecords_features['Homo'] = tf.train.Feature(bytes_list=tf.train.BytesList(value=homo_list))
    tfrecords_features['shape'] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(shape)))
    tfrecords_features['m_num'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[m_num]))
    return tf.train.Example(features=tf.train.Features(feature=tfrecords_features))

class Trajectory(object):
    def __init__(self, canvas=140, iters=2000, max_len=200, expl=None, path_to_save=None):#1000->2000
        """
        Generates a variety of random motion trajectories in continuous domain as in [Boracchi and Foi 2012]. Each
        trajectory consists of a complex-valued vector determining the discrete positions of a particle following a
        2-D random motion in continuous domain. The particle has an initial velocity vector which, at each iteration,
        is affected by a Gaussian perturbation and by a deterministic inertial component, directed toward the
        previous particle position. In addition, with a small probability, an impulsive (abrupt) perturbation aiming
        at inverting the particle velocity may arises, mimicking a sudden movement that occurs when the user presses
        the camera button or tries to compensate the camera shake. At each step, the velocity is normalized to
        guarantee that trajectories corresponding to equal exposures have the same length. Each perturbation (
        Gaussian, inertial, and impulsive) is ruled by its own parameter. Rectilinear Blur as in [Boracchi and Foi
        2011] can be obtained by setting anxiety to 0 (when no impulsive changes occurs
        :param canvas: size of domain where our trajectory os defined.
        :param iters: number of iterations for definition of our trajectory.
        :param max_len: maximum length of our trajectory.
        :param expl: this param helps to define probability of big shake. Recommended expl = 0.005.
        :param path_to_save: where to save if you need.
        """
        self.canvas = canvas
        self.iters = iters
        self.max_len = max_len
        if expl is None:
            self.expl = 0.1 * np.random.uniform(0, 1)
        else:
            self.expl = expl
        if path_to_save is None:
            pass
        else:
            self.path_to_save = path_to_save
        self.tot_length = None
        self.big_expl_count = None
        self.tot_length2 = None
        self.big_expl_count2 = None
        self.tot_length3 = None
        self.big_expl_count3 = None
        self.tot_length4 = None
        self.big_expl_count4 = None      
        
        self.x = None
        self.y = None
        self.z = None
        self.w = None
        

    def fit(self, show=False, save=False):
        """
        Generate motion, you can save or plot, coordinates of motion you can find in x property.
        Also you can fin properties tot_length, big_expl_count.
        :param show: default False.
        :param save: default False.
        :return: x (vector of motion).
        """
        tot_length = 0
        big_expl_count = 0
        tot_length2 = 0
        big_expl_count2 = 0
        tot_length3 = 0
        big_expl_count3 = 0
        tot_length4 = 0
        big_expl_count4 = 0
        # how to be near the previous position
        # TODO: I can change this paramether for 0.1 and make kernel at all image
        centripetal = 0.7 * np.random.uniform(0, 1)
        # probability of big shake
        prob_big_shake = 0.2 * np.random.uniform(0, 1)#original is 0.2
        # term determining, at each sample, the random component of the new direction
        gaussian_shake = 10 * np.random.uniform(0, 1)
        init_angle = 360 * np.random.uniform(0, 1)
        init_angle2 = 360 * np.random.uniform(0, 1)
        init_angle3 = 360 * np.random.uniform(0, 1)
        init_angle4 = 360 * np.random.uniform(0, 1)

        img_v0 = np.sin(np.deg2rad(init_angle))
        real_v0 = np.cos(np.deg2rad(init_angle))

        img_v02 = np.sin(np.deg2rad(init_angle2))
        real_v02 = np.cos(np.deg2rad(init_angle2))
        
        img_v03 = np.sin(np.deg2rad(init_angle3))
        real_v03 = np.cos(np.deg2rad(init_angle3))
        
        img_v04 = np.sin(np.deg2rad(init_angle4))
        real_v04 = np.cos(np.deg2rad(init_angle4))
        
        v0 = complex(real=real_v0, imag=img_v0)
        v = v0 * self.max_len / (self.iters - 1)
        
        v02 = complex(real=real_v02, imag=img_v02)
        v2 = v02 * self.max_len / (self.iters - 1)

        v03 = complex(real=real_v03, imag=img_v03)
        v3 = v03 * self.max_len / (self.iters - 1)

        v04 = complex(real=real_v04, imag=img_v04)
        v4 = v04 * self.max_len / (self.iters - 1)

        if self.expl > 0:
            v = v0 * self.expl
            v2 = v02 * self.expl
            v3 = v03 * self.expl
            v4 = v04 * self.expl


        x = np.array([complex(real=0, imag=0)] * (self.iters))
        y = np.array([complex(real=0, imag=0)] * (self.iters))
        z = np.array([complex(real=0, imag=0)] * (self.iters))
        w = np.array([complex(real=0, imag=0)] * (self.iters))
        if (np.random.randint(2,size=1)[0] == 1):
            flag=1
        else:
            flag=-1

        for t in range(0, frm_num+1):
            if np.random.uniform() < prob_big_shake * self.expl:
                next_direction = 2 * v * (np.exp(complex(real=0, imag=np.pi + (np.random.uniform() - 0.5))))
                big_expl_count += 1
            else:
                next_direction = 0

            dv = next_direction + self.expl * (
                gaussian_shake * complex(real=np.random.randn(), imag=np.random.randn()) - centripetal * x[t]) * (
                                      self.max_len / (self.iters - 1))

            v += dv
            v = (v / float(np.abs(v))) * (self.max_len / float((self.iters - 1)))
            if ((np.random.randint(2,size=1)[0] == 1)and (t%20==0)):   #(np.random.randint(2,size=1)[0] == 1) and   and (t%20==0)
                flag=-1*flag
                #x[t]=complex(real=0,imag=0)
            x[t + 1] = x[t] + flag*v
            #print(flag)
            tot_length = tot_length + abs(x[t + 1] - x[t])
        #print(x)    
        x+=complex(real=20,imag=20)

        # centere the motion
        #x += complex(real=-np.min(x.real), imag=-np.min(x.imag))
        #x = x - complex(real=x[0].real % 1., imag=x[0].imag % 1.) + complex(1, 1)
        #x += complex(real=ceil((self.canvas - max(x.real)) / 2), imag=ceil((self.canvas - max(x.imag)) / 2))

        self.tot_length = tot_length
        self.big_expl_count = big_expl_count
        self.x = x
        
        
        for t in range(0, frm_num+1):
            if np.random.uniform() < prob_big_shake * self.expl:
                next_direction2 = 2 * v2 * (np.exp(complex(real=0, imag=np.pi + (np.random.uniform() - 0.5))))
                big_expl_count2 += 1
            else:
                next_direction2 = 0

            dv2 = next_direction2 + self.expl * (
                gaussian_shake * complex(real=np.random.randn(), imag=np.random.randn()) - centripetal * y[t]) * (
                                      self.max_len / (self.iters - 1))

            v2 += dv2
            v2 = (v2 / float(np.abs(v2))) * (self.max_len / float((self.iters - 1)))
            if ((np.random.randint(2,size=1)[0] == 1) and (t%20==0)):   
                flag=-1*flag
                #y[t]=complex(real=0,imag=0)
            y[t + 1] = y[t] + flag*v2
            tot_length2 = tot_length2 + abs(y[t + 1] - y[t])
        y+=complex(real=20,imag=120)

        #print(y)    

        # centere the motion
        #y += complex(real=-np.min(y.real), imag=-np.min(y.imag))
        #y = y - complex(real=y[0].real % 1., imag=y[0].imag % 1.) + complex(1, 1)
        #y += complex(real=ceil((self.canvas - max(y.real)) / 2), imag=ceil((self.canvas - max(y.imag)) / 2))
        
        self.tot_length2 = tot_length2
        self.big_expl_count2 = big_expl_count2
        self.y = y


        for t in range(0, frm_num+1):
            if np.random.uniform() < prob_big_shake * self.expl:
                next_direction3 = 2 * v3 * (np.exp(complex(real=0, imag=np.pi + (np.random.uniform() - 0.5))))
                big_expl_count3 += 1
            else:
                next_direction3 = 0

            dv3 = next_direction3 + self.expl * (
                gaussian_shake * complex(real=np.random.randn(), imag=np.random.randn()) - centripetal * z[t]) * (
                                      self.max_len / (self.iters - 1))

            v3 += dv3
            v3 = (v3 / float(np.abs(v3))) * (self.max_len / float((self.iters - 1)))
            if ((np.random.randint(2,size=1)[0] == 1) and (t%20==0)):   
                flag=-1*flag
                #z[t]=complex(real=0,imag=0)
            z[t + 1] = z[t] + flag*v3
            tot_length3 = tot_length3 + abs(z[t + 1] - z[t])
        z+=complex(real=120,imag=120)

        #print(z)    
        # centere the motion
        #z += complex(real=-np.min(z.real), imag=-np.min(z.imag))
        #z = z - complex(real=z[0].real % 1., imag=z[0].imag % 1.) + complex(1, 1)
        #z += complex(real=ceil((self.canvas - max(z.real)) / 2), imag=ceil((self.canvas - max(z.imag)) / 2))

        self.tot_length3 = tot_length3
        self.big_expl_count3 = big_expl_count3
        self.z = z

        for t in range(0, frm_num+1):
            if np.random.uniform() < prob_big_shake * self.expl:
                next_direction4 = 2 * v4 * (np.exp(complex(real=0, imag=np.pi + (np.random.uniform() - 0.5))))
                big_expl_count4 += 1
            else:
                next_direction4 = 0

            dv4 = next_direction4 + self.expl * (
                gaussian_shake * complex(real=np.random.randn(), imag=np.random.randn()) - centripetal * w[t]) * (
                                      self.max_len / (self.iters - 1))

            v4 += dv4
            v4 = (v4 / float(np.abs(v4))) * (self.max_len / float((self.iters - 1)))
            if ((np.random.randint(2,size=1)[0] == 1)and (t%20==0)):   
                flag=-1*flag
                #w[t]=complex(real=0,imag=0)
            w[t + 1] = w[t] + flag*v4
            tot_length4 = tot_length4 + abs(w[t + 1] - w[t])
        w+=complex(real=120,imag=20)

        #print(w)

        # centere the motion
        #w += complex(real=-np.min(w.real), imag=-np.min(w.imag))
        #w = w - complex(real=w[0].real % 1., imag=w[0].imag % 1.) + complex(1, 1)
        #w += complex(real=ceil((self.canvas - max(w.real)) / 2), imag=ceil((self.canvas - max(w.imag)) / 2))
        self.tot_length4 = tot_length4
        self.big_expl_count4 = big_expl_count4
        self.w = w

        im = np.float32(cv2.imread('2.jpg'))
        sum = 0
        sum2 =0
        sum3=0
        sum4=0
        im2=im
        sp=im.shape
        #print(sp)
        size = (sp[1],sp[0])
        Htotal = np.stack([np.eye(3)]*frm_num,axis=0)
        H = np.stack([np.eye(3)]*(frm_num+4),axis=0)
        
        src_p1 = np.float32([[x[0].real,x[0].imag],[y[0].real,y[0].imag],[z[0].real,z[0].imag],[w[0].real,w[0].imag]])
        #src_p2 = np.float32([[x[60].real,x[60].imag],[y[60].real,y[60].imag],[z[60].real,z[60].imag]])
        theta=0
        flag1=-1
        for t in range(0, frm_num): #
            #src_point = np.float32([[x[t].real,x[t].imag],[y[t].real,y[t].imag],[z[t].real,z[t].imag],[w[t].real,w[t].imag]])
            #print(src_point)
            dst_point = np.float32([[x[t+1].real,x[t+1].imag],[y[t+1].real,y[t+1].imag],[z[t+1].real,z[t+1].imag],[w[t+1].real,w[t+1].imag]])
            #dst_p2 = np.float32([[x[t+61].real,x[t+61].imag],[y[t+61].real,y[t+61].imag],[z[t+61].real,z[t+61].imag]])

            #print(dst_point)
            #h,s = cv2.findHomography(src_point, dst_point, cv2.RANSAC, 5)
            #H[t]=h
            Htotal[t],_ = cv2.findHomography(src_p1, dst_point, cv2.RANSAC, 5)
            Htotal[t][2][0:2]=0
            
            #tant=-Htotal[t][0][1]/Htotal[t][0][0]
            #theta=np.arctan(tant)
            if((np.random.randint(2,size=1)[0] == 1) and (t%5==0)):   #(np.random.randint(2,size=1)[0] == 1) and   and (t%20==0)
                flag1=-flag1
            print(flag1)
            theta=theta+flag1*0*np.random.uniform(0, 1)/self.iters#90                
                #if(t%10==0):
            #    scale=Htotal[t][0][0]/np.cos(theta)
            #else:
            #    scale=1
            #Htotal[t][0][1]=0
            scale=1
            #theta=0
            Htotal[t][0][0]=scale*np.cos(np.deg2rad(theta))
            Htotal[t][0][1]=scale*(-np.sin(np.deg2rad(theta)))
            Htotal[t][1][0]=-Htotal[t][0][1]
            #Htotal[t][0][0]=1
            Htotal[t][1][1]=Htotal[t][0][0]
            #Htotal[t][0][2]=0
            #Htotal[t][1][2]=0
            #print(Htotal[t])

        for i in range(20):
            H[i]=Htotal[i]
        H[20]=np.linalg.inv(Htotal[0])
        for j in range(21,41):
            H[j]=Htotal[j-1]
        H[41]=np.linalg.inv(Htotal[20])
        for k in range(42,62):
            H[k]=Htotal[k-2]
        H[62]=np.linalg.inv(Htotal[40])
        for l in range(63,83):
            H[l]=Htotal[l-3]
        H[83]=np.linalg.inv(Htotal[60])         
        
        return H
 
            





if __name__ == '__main__':
    dst_path = "/DATA1/dlnr_zcz/homo/simulate/3/"#no rotation
    create_dir(dst_path)
    print(dst_path)
    number = 400000
    iter_num = 0
    t_num=frm_num+4
    start_time = time.time()
    example_num= 50000
    record_start_cnt= 0
    record_num = number//example_num
    if record_num * example_num != number:
        record_num += 1

    for i in range(record_num):
        tfrecord_name = dst_path + "N%05d.tfrecord" % (i + 1 + record_start_cnt)
        writer = tf.python_io.TFRecordWriter(tfrecord_name)
        for j in range(min(number - example_num * i, example_num)):
            
            trajectory = Trajectory(expl=0.005,
                        path_to_save="/DATA1/dlnr_zcz/trajectory/")
            H=trajectory.fit(True, False)
            H = H.reshape(t_num,9)
            H = np.float32(H[:,0:8])
            print(H.shape)
            #print(H)
            print("process %d in %d"%(example_num * i+j, number))
            H_bytes = H.tobytes()
            example = gen_example_v2([H_bytes], shape=(t_num,8),m_num=m_num) 
            example_serial = example.SerializeToString() 
            writer.write(example_serial)               
        writer.close()
        print("iter: %d, time: %4.4fs " %(iter_num, time.time()-start_time))
        iter_num = iter_num+1    
'''
            im2=cv2.warpPerspective(im, Htotal[t],size)
            cv2.imwrite('%s.png'%t,im2)
            if t<m_num:
                sum+=im2
            if ((t<2*m_num) and (t>=m_num)):
                sum2=sum2+im2
            if((t<3*m_num) and(t>=2*m_num)):
                sum3+=im2
            if((t<4*m_num) and (t>=3*m_num)):
                sum4+=im2
                
        sum/=m_num
        sum2/=m_num
        sum3/=m_num
        sum4/=m_num
        
        cv2.imwrite('81.png',sum)
        cv2.imwrite('82.png',sum2)
        cv2.imwrite('83.png',sum3)
        cv2.imwrite('84.png',sum4)        
        
        sum2=cv2.warpPerspective(sum2,np.linalg.inv(Htotal[20]),size)
        sum3=cv2.warpPerspective(sum3,np.linalg.inv(Htotal[2*20]),size)
        sum4=cv2.warpPerspective(sum4,np.linalg.inv(Htotal[3*20]),size)



        cv2.imwrite('91.png',sum)
        cv2.imwrite('92.png',sum2)
        cv2.imwrite('93.png',sum3)
        cv2.imwrite('94.png',sum4)
        
        if show or save:
            self.__plot_canvas(show, save)
        return self

    def __plot_canvas(self, show, save):
        if self.x is None:
            raise Exception("Please run fit() method first")
        else:
            plt.close()
            plt.plot(self.x.real, self.x.imag, '-', color='blue')
            plt.plot(self.y.real, self.y.imag, '-', color='red')
            plt.plot(self.z.real, self.z.imag, '-', color='green')
            plt.plot(self.w.real, self.w.imag, '-', color='orange')
            
            plt.xlim((0, self.canvas))
            plt.ylim((0, self.canvas))
            if show and save:
                plt.savefig(self.path_to_save)
                plt.show()
            elif save:
                if self.path_to_save is None:
                    raise Exception('Please create Trajectory instance with path_to_save')
                plt.savefig(self.path_to_save)
            elif show:
                plt.show()
if __name__ == '__main__':
    trajectory = Trajectory(expl=0.005,
                            path_to_save="/DATA1/dlnr_zcz/trajectory/")
    trajectory.fit(True, False)
'''

#########read_and_calc_homo.py###################
import numpy as np
from math import ceil
import matplotlib.pyplot as plt
import cv2,os
import glob,time
import tensorflow as tf

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def read_videos(src_path,dst_path):
    create_dir(dst_path)
    src_files = glob.glob(src_path+"*.mp4")
    for src_pa in src_files:
        video_path, full_name = os.path.split(src_pa)
        video_name,video_attri = os.path.splitext(full_name)
        pic_path = dst_path +'/'+ video_name 
        create_dir(pic_path)
        videoCapture = cv2.VideoCapture(src_pa)
        size = (int(videoCapture.get(3)*0.5),int(videoCapture.get(4)*0.5))
        frame_cnt = 1
        success,frame = videoCapture.read()
        while success:
            cv2.imshow("vid",frame)
            frame = cv2.resize(frame,size,interpolation=cv2.INTER_AREA)
            cv2.imwrite(pic_path+"/%06d.png" % frame_cnt,frame)
            success,frame = videoCapture.read()
            frame_cnt = frame_cnt+1  






if __name__ == '__main__':
    
    src_path = "/DATA1/dlnr_zcz/videos/"
    dst_path = "/DATA1/dlnr_zcz/pictures/"    
    read_videos(src_path,dst_path)

#############calc_homo.pt##################

import numpy as np
from math import ceil
import matplotlib.pyplot as plt
import cv2,os
import glob,time
import tensorflow as tf

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def gen_example(homo_list, shape=(12, 8), m_num=1):
    tfrecords_features = {}
    tfrecords_features['Homo'] = tf.train.Feature(bytes_list=tf.train.BytesList(value=homo_list))
    tfrecords_features['shape'] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(shape)))
    tfrecords_features['m_num'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[m_num]))
    return tf.train.Example(features=tf.train.Features(feature=tfrecords_features))
	

def gen_example_v2(homo_list, shape=(1, 8), m_num=1):
    tfrecords_features = {}
    tfrecords_features['Homo'] = tf.train.Feature(bytes_list=tf.train.BytesList(value=homo_list))
    tfrecords_features['shape'] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(shape)))
    tfrecords_features['m_num'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[m_num]))
    return tf.train.Example(features=tf.train.Features(feature=tfrecords_features))


def calc_matrix(img1,img2):
    #first, find the sift features in images and apply the ratio test to find the best matches
    MIN_MATCH_COUNT = 4
    sift = cv2.xfeatures2d.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1,None)
    kp2, des2 = sift.detectAndCompute(img2,None)
    FLANN_INDEX_KDTREE = 0
    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
    search_params = dict(check = 50)
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matchs = flann.knnMatch(des1,des2,k=2)	
    good = []
    for m,n in matchs:
    	if m.distance < 0.7*n.distance:
    		good.append(m)
    	
    #extract the locations of matched keypoints in both the images, and get the 3*3 transformation matrix
    if len(good)>MIN_MATCH_COUNT:
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)
        return M
        #M = M.reshape(9,1)
        #M = M[0:8]    	

def gen_tfrecord(src_rt_files, dst_path):
    create_dir(dst_path)
    iter_num = 0
    for src_pa in src_rt_files:
        start_time = time.time()
        src_files = glob.glob(src_pa+"*.png")
        src_num = len(src_files)
        print(src_num)
        total_h = src_num#*15
        pic_start_path = sorted(src_files)[0]
        pic_path,pic_name = os.path.split(pic_start_path)
        print(pic_path,pic_name)
        pic_number,pic_class = os.path.splitext(pic_name)
        pic_number = int(pic_number)
        H00 = np.diag(np.diag(np.ones([3,3]))) #use 48 images to generate 48 homography 8*6
        H01 = np.stack([H00]*total_h,axis=0)
        for x in range(1):#3
            for y in range(1):#5
                for src in range(1,src_num):#min(src_num,1000)):
                    img1=cv2.imread(src_pa+"%06d.png" % (src+pic_number-1),0)
                    size=(640,360)
                    img1 = cv2.resize(img1,size,interpolation=cv2.INTER_AREA)
                    img2=cv2.imread(src_pa+"%06d.png" % (src+pic_number),0)
                    img2 = cv2.resize(img2,size,interpolation=cv2.INTER_AREA)
                    H00_tmp = calc_matrix(img1,img2)#calc_matrix(img1[240*x:240*(x+1),320*y:320*(y+1)],img2[240*x:240*(x+1),320*y:320*(y+1)])#
                    #print(x*src_num+y*src_num+src)
                    print(src)
                    if (H00_tmp is not None):
                        H01[x*1*src_num+y*src_num+src] = H00_tmp                        
        print(len(H01))
        t_max = 1# 11#9#11#13
        #frm_num = 6
        crop_num = 1
        number = crop_num*(src_num//t_max)
        example_num= src_num//t_max
        record_start_cnt= iter_num * crop_num#+86
        record_num = number // example_num
        if record_num * example_num != number:
            record_num += 1

        for i in range(record_num):
            tfrecord_name = dst_path + "N%05d.tfrecord" % (i + 1 + record_start_cnt)
            writer = tf.python_io.TFRecordWriter(tfrecord_name)
            for j in range(min(number - example_num * i, example_num,total_h-t_max)):
                #m_num = np.random.choice([1,2,3,4],p=[0.3,0.45,0.2,0.05])
                m_num = 1
                t_num = 1#frm_num+m_num-1
                #t_num = m_num * frm_num
                H0 = np.diag(np.diag(np.ones([3,3]))) #use 48 images to generate 48 homography 8*6
                H = np.stack([H0]*t_num,axis=0)
                for k in range(t_num):
                    H_tmp = H01[i*src_num+j*t_num+k] 
                    #print('H_tmp\n',H_tmp )
                    H[k] = H_tmp#np.dot(H_tmp,H[k-1])
                    #print('H[%d]\n'%k,H[k])
                H = H.reshape(t_num,9)
                H = np.float32(H[:,0:8])
                #print(H)
                print("process %d in %d"%(example_num * i+j, number))
                #print("picture %d" % ((k + 1 + j*t_num + i*example_num*t_num)))
                #print(H.shape)
                H_bytes = H.tobytes()
                example = gen_example_v2([H_bytes], shape=(t_num,8),m_num=m_num) 
                example_serial = example.SerializeToString() 
                writer.write(example_serial)               
            writer.close()
        print("iter: %d, time: %4.4fs " %(iter_num, time.time()-start_time))
        iter_num = iter_num+1

'''            
        for i in range(record_num):
            tfrecord_name = dst_path + "N%05d.tfrecord" % (i + 1 + record_start_cnt)
            writer = tf.python_io.TFRecordWriter(tfrecord_name)
            for j in range(min(number - example_num * i, example_num,total_h-t_max)):
                #m_num = np.random.choice([1,2,3,4],p=[0.3,0.45,0.2,0.05])
                m_num = 4
                t_num = 9#frm_num+m_num-1
                #t_num = m_num * frm_num
                H0 = np.diag(np.diag(np.ones([3,3]))) #use 48 images to generate 48 homography 8*6
                H = np.stack([H0]*t_num,axis=0)
                for k in range(1,t_num):
                    H_tmp = H01[i*src_num+j*t_num+k] 
                    #print('H_tmp\n',H_tmp )
                    H[k] = H_tmp#np.dot(H_tmp,H[k-1])
                    #print('H[%d]\n'%k,H[k])
                H = H.reshape(t_num,9)
                H = np.float32(H[:,0:8])
                #print(H)
                print("process %d in %d"%(example_num * i+j, number))
                #print("picture %d" % ((k + 1 + j*t_num + i*example_num*t_num)))
                #print(H.shape)
                H_bytes = H.tobytes()
                example = gen_example([H_bytes], shape=(t_num,8),m_num=m_num) 
                example_serial = example.SerializeToString() 
                writer.write(example_serial)               
            writer.close()
        print("iter: %d, time: %4.4fs " %(iter_num, time.time()-start_time))
        iter_num = iter_num+1
'''
    


if __name__ == '__main__':
    
    src_path = "/DATA1/dlnr_zcz/train/"
    #src_path = "/DATA1/dlnr_zcz/pictures/"
    #dst_path = "/DATA1/dlnr_zcz/homo/crop/self/"
    dst_path = "/DATA1/dlnr_zcz/homo/new/"
    #dst_path = "/DATA1/dlnr_zcz/homo/crop/train_8/"  #8 images 4+10 or 6+8
    #dst_path = "/DATA1/dlnr_zcz/homo/crop/train_6/"  #6 images 4+8 or 6+6 
    #dst_path = "/DATA1/dlnr_zcz/homo/crop/train_4/"  #4 images 4+6 or 6+4
    #src_path = "/DATA1/dlnr_zcz/test/"
    #dst_path = "/DATA1/dlnr_zcz/homo/test/"    
    src_rt_files = glob.glob(src_path+'*/')
    #print(len(src_rt_files))
    #print(sorted(src_rt_files)[0])
    gen_tfrecord(src_rt_files,dst_path)


